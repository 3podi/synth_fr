sft_config:
  _target_: trl.SFTConfig
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  logging_strategy: "epoch"
  logging_steps: 1
  report_to: "wandb"
  #logging_dir: "logs/"
  remove_unused_columns: true
  learning_rate: 1e-5
  warmup_ratio: 0.1
  group_by_length: true
  max_seq_length: 512
  num_train_epochs: 10
  save_safetensors: false
  save_strategy: "epoch"
  output_dir: "lora/sft"
  label_names: ["labels"]
  dataset_kwargs:
    skip_prepare_dataset: true
  
model_config:
  _target_: trl.ModelConfig
  attn_implementation: 'eager' # use 'flash_attention_2' if available
  use_peft: true
  torch_dtype: "bfloat16"
  lora_r: 32
  lora_alpha: 64
  lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  model_name_or_path: ???

dataset: ???
tags: ???
group_id: ???
run_id: ???
adapters_paths: ???
dataset_size: ??? 
