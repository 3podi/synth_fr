sft_config:
  _target_: trl.SFTConfig
  per_device_train_batch_size: 3
  gradient_accumulation_steps: 2
  logging_steps: 10
  remove_unused_columns: true
  learning_rate: 5e-5
  warmup_ratio: 0
  group_by_length: true
  max_seq_length: 512
  num_train_epochs: 1
  save_safetensors: false
  save_strategy: "epoch"
  report_to: "wandb"
  output_dir: "lora/sft"

model_config:
  _target_: trl.ModelConfig
  use_peft: true
  torch_dtype: "bfloat16"
  lora_r: 32
  lora_alpha: 64
  lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  #model_name_or_path: meta-llama/llama-2-7b-hf
  model_name_or_path: mistralai/Mistral-7B-v0.1

dataset: ???
tags: ???
group_id: ???
run_id: ???
adapters_paths: ???
dataset_size: 1000
