model_config:
  model_name_or_path: "answerdotai/ModernBERT-large"
  #model_name_or_path: "microsoft/deberta-v3-base"
  torch_dtype: "bfloat16"
  classifier_dropout: 0.5
  mlp_dropout: 0.5
  attention_dropout: 0.3

training_arguments:
  learning_rate: 1e-4
  num_train_epochs: 10
  warmup_steps: 300
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  logging_steps: 10
  eval_strategy: "steps"
  eval_steps: 50
  save_steps: 10000
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  report_to: "wandb"
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  max_grad_norm: 10.0

max_seq_length: 2048

# Data
dataset_size: -1  # -1 means use all data
text_column: "response"  # Column name for input text
label_column: "codes"  # Column name for labels (comma-separated)
label_filter_percentage: null
label_filter_number: 50
max_labels_per_sample: 10

# File paths
dataset: ../train_classification2.parquet  # Path to your parquet dataset
val_dataset_2: ../validation2.parquet

# Save model
save_flag: false

run_id: prova
tags: ["multilabel", "classification"]
group_id: "multilabel_group"