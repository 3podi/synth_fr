dpo_config:
  _target_: trl.DPOConfig
  beta: 0.1
  bf16: true
  eval_strategy: "epoch" #"no"
  load_best_model_at_end: True
  metric_for_best_model: "eval_loss"
  gradient_checkpointing: true
  group_by_length: true
  learning_rate: 5e-6
  logging_strategy: "steps"
  logging_steps: 10
  weight_decay: 1e-7
  max_length: 1024 #512
  num_train_epochs: 5
  output_dir: "models/dpo/"
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  truncation_mode: keep_start
  remove_unused_columns: false
  save_only_model: true
  save_safetensors: false
  save_strategy: "epoch" #"no"
  model_adapter_name: "default"
  ref_adapter_name: "reference"
  seed: 0
  warmup_ratio: 0

peft_config:
  _target_: peft.LoraConfig
  r: 32
  lora_alpha: 64
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

model_config:
  _target_: trl.ModelConfig
  torch_dtype: "bfloat16"
  model_name_or_path: ???

dataset: ???
dataset_size: 900
tags: ???
iteration: ???
adapters_paths: ???
group_id: ???
run_id: ???
